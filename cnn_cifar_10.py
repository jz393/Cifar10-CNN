# -*- coding: utf-8 -*-
"""CNN cifar-10

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LvuhpqxfPHFWrdRxz5FfAIMyP-K0bIgA
"""

import numpy as np
import tensorflow as tf
from sklearn.utils import shuffle

#load in data
from tensorflow.keras.datasets.cifar10 import load_data
cifar_data = load_data()

img_train = cifar_data[0][0]
label_train = cifar_data[0][1]

img_test = cifar_data[1][0]
label_test = cifar_data[1][1]

label_train_oh = []
label_test_oh = []

for i in range(len(label_train)):
  new_vec = np.zeros(10)
  new_vec[label_train[i]] = 1
  label_train_oh.append(new_vec)
label_train_oh = np.array(label_train_oh)

for i in range(len(label_test)):
  new_vec = np.zeros(10)
  new_vec[label_train[i]] = 1
  label_test_oh.append(new_vec)
label_test_oh = np.array(label_test_oh)

input_ph = tf.placeholder(tf.float32, shape=[None, 32, 32, 3], name='input_image')

kernel1 = tf.get_variable('kernel1', [3, 3, 3, 64], dtype=tf.float32, initializer=tf.random_normal_initializer)
conv1 = tf.nn.conv2d(input_ph, kernel1, [1, 1, 1, 1], 'SAME')

bias1 = tf.Variable(tf.zeros([64]), name='bias1')
relu1 = tf.nn.relu(tf.nn.bias_add(conv1, bias1), name='relu1')

kernel2 = tf.get_variable('kernel2', [3, 3, 64, 64], dtype=tf.float32, initializer=tf.random_normal_initializer)
conv2 = tf.nn.conv2d(relu1, kernel2, [1, 1, 1, 1], 'SAME')
bias2 = tf.Variable(tf.zeros([64]), name='bias2')
relu2 = tf.nn.relu(tf.nn.bias_add(conv2, bias2), name='relu2')

maxpool1 = tf.nn.max_pool(relu2, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='maxpool1')

kernel3 = tf.get_variable('kernel3', [3, 3, 64, 128], dtype=tf.float32, initializer=tf.random_normal_initializer)
conv3 = tf.nn.conv2d(maxpool1, kernel3, [1, 1, 1, 1], 'SAME')
bias3 = tf.Variable(tf.zeros([128]), name='bias3')
relu3 = tf.nn.relu(tf.nn.bias_add(conv3, bias3), name='relu3')

kernel4 = tf.get_variable('kernel4', [3 ,3, 128, 128], dtype=tf.float32, initializer=tf.random_normal_initializer)
conv4 = tf.nn.conv2d(relu3, kernel4, [1, 1, 1, 1], 'SAME')
bias4 = tf.Variable(tf.zeros([128]), name='bias4')
relu4 = tf.nn.relu(tf.nn.bias_add(conv4, bias4), name='relu4')

maxpool2 = tf.nn.max_pool(relu4, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='maxpool2')

kernel5 = tf.get_variable('kernel5', [3, 3, 128, 256], dtype=tf.float32, initializer = tf.random_normal_initializer)

conv5 = tf.nn.conv2d(maxpool2, kernel5, [1, 1, 1, 1], 'SAME')
bias5 = tf.Variable(tf.zeros([256]), name = 'bias5')
relu5 = tf.nn.relu(tf.nn.bias_add(conv5, bias5), name='relu5')

kernel6 = tf.get_variable('kernel6', [3, 3, 256, 256], dtype=tf.float32, initializer=tf.random_normal_initializer)
conv6 = tf.nn.conv2d(relu5, kernel6, [1, 1, 1, 1], 'SAME')
bias6 = tf.Variable(tf.zeros([256]), name='bias6')
relu6 = tf.nn.relu(tf.nn.bias_add(conv6, bias6), name='relu6')

maxpool3 = tf.nn.max_pool(relu6, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME', name='maxpool3')

flatten = tf.reshape(maxpool3, [-1, 4 * 4 * 256])

fc_w_1 = tf.get_variable('fc_w_1', [4096, 4096], dtype=tf.float32, initializer=tf.random_normal_initializer)
fc_b_1 = tf.Variable(tf.zeros([4096]), name='fc_b_1')
linear = tf.matmul(flatten, fc_w_1) + fc_b_1

fc_relu_1 = tf.nn.relu(linear, name='fc1')

fc_w_2 = tf.get_variable('fc_w_2', [4096, 10], dtype=tf.float32, initializer=tf.random_normal_initializer)
fc_b_2 = tf.get_variable('fc_b_1', dtype=tf.float32, initializer=tf.zeros([10]))
output = tf.matmul(fc_relu_1, fc_w_2) + fc_b_2

#setup loss function
output_ph = tf.placeholder(tf.float32, shape=[None, 10])
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=output_ph, logits=output))

optim_op = tf.train.AdamOptimizer().minimize(loss)


sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())

batch_size = 64
epochs = 50
for i in range(epochs):
  img_train, label_train_oh = shuffle(img_train, label_train_oh)
  batch_idx = len(img_train) // batch_size
  for j in range(batch_idx):
    batch_x = img_train[j * batch_size : (j + 1) * batch_size]
    batch_y = label_train_oh[j * batch_size : (j + 1) * batch_size]
    
    feed_dict = {input_ph : batch_x, output_ph : batch_y}
    sess.run(optim_op, feed_dict=feed_dict)
    
    if j % 100 == 0:
      batch_loss = sess.run(loss, feed_dict)
      print(batch_loss)

